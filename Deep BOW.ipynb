{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Continuous Bag of Words\n",
    "\n",
    "![alt text](src/arch.jpg)\n",
    "\n",
    "source: https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import random\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=400000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Create word2vec dictionary\n",
    "\n",
    "def load_word_vectors(filename):\n",
    "    word_to_index = {}\n",
    "    word_vectors  = []\n",
    "    \n",
    "    with open(filename, encoding=\"utf8\") as fp:\n",
    "        for line in tqdm_notebook(fp.readlines(), leave = False):\n",
    "            line = line.split(\" \")\n",
    "            \n",
    "            word = line[0]\n",
    "            word_to_index[word] = len(word_to_index)\n",
    "            \n",
    "            vec = np.array([float(x) for x in line[1:]])\n",
    "            word_vectors.append(vec)\n",
    "            \n",
    "    return word_to_index, word_vectors\n",
    "\n",
    "word_to_index, word_vectors = load_word_vectors(\"model/glove.6B.100d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Right on the money</td>\n",
       "      <td>We are using the this book to get 100+ certifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Serves its Purpose!</td>\n",
       "      <td>Couldn't go without it. My 3 1/2 year still we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Trailer Park Bwoys!!!</td>\n",
       "      <td>we get to see it on paramount in ol' LND UK an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>buyer beware</td>\n",
       "      <td>There are companies selling Bosch knock-offs o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Great for those cold winters</td>\n",
       "      <td>If you are looking to keep your water liquifie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                         title  \\\n",
       "0          2            Right on the money   \n",
       "1          2           Serves its Purpose!   \n",
       "2          2         Trailer Park Bwoys!!!   \n",
       "3          1                  buyer beware   \n",
       "4          2  Great for those cold winters   \n",
       "\n",
       "                                             content  \n",
       "0  We are using the this book to get 100+ certifi...  \n",
       "1  Couldn't go without it. My 3 1/2 year still we...  \n",
       "2  we get to see it on paramount in ol' LND UK an...  \n",
       "3  There are companies selling Bosch knock-offs o...  \n",
       "4  If you are looking to keep your water liquifie...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import csv file of amazon reviews\n",
    "\n",
    "df = pd.read_csv(\"data/amazon_reviews_small.csv\", names = ['sentiment', 'title', 'content'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(phrase):\n",
    "    phrase = phrase.split(' ')\n",
    "    vector = np.zeros((1,100))\n",
    "    word_count = 0\n",
    "    for word in phrase:\n",
    "        try:\n",
    "            vector += np.asarray(word_vectors[word_to_index[word]])\n",
    "            word_count += 1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if word_count > 0:\n",
    "        ans = vector/word_count\n",
    "    else:\n",
    "        ans = np.zeros((1,100))\n",
    "    \n",
    "    return ans\n",
    "    \n",
    "def clean_phrase(phrase):\n",
    "    phrase = phrase.lower()\n",
    "    phrase = phrase.replace(\"'\", \"\")\n",
    "    phrase = phrase.replace(\".\", \"\")\n",
    "    phrase = re.sub(r'([^\\s\\w]|_)+', '', phrase)\n",
    "    return phrase\n",
    "\n",
    "def generate_phrases(dataset):\n",
    "    phrases = dataset[['title', 'content']].dropna().values.flatten().tolist()\n",
    "    return phrases\n",
    "    \n",
    "def generate_tfidf(phrases):\n",
    "    tfidf = TfidfVectorizer()\n",
    "    tfidf.fit(phrases)\n",
    "    return tfidf\n",
    "\n",
    "def phrase_cbow(a, b):\n",
    "    aa = a.reshape(1,-1)\n",
    "    ba = b.reshape(1,-1)\n",
    "    \n",
    "    cos_lib = cosine_similarity(aa, ba)\n",
    "\n",
    "    return cos_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment']     = [(a-1) for a in df.sentiment.values]\n",
    "df['title_clean']   = [clean_phrase(str(a)) for a in df.title]\n",
    "df['content_clean'] = [clean_phrase(str(a)) for a in df.content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = generate_tfidf(generate_phrases(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating clean title\n",
      "Generating clean content\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating clean title\")\n",
    "df['title_rep']     = [tfidf.transform([a]) for a in df.title_clean]\n",
    "print(\"Generating clean content\")\n",
    "df['content_rep']   = [tfidf.transform([a]) for a in df.content_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating clean title vector\n",
      "Generating clean content vector\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating clean title vector\")\n",
    "df['title_vec']     = [get_vectors(a) for a in df.title_clean]\n",
    "print(\"Generating clean content vector\")\n",
    "df['content_vec']   = [get_vectors(a) for a in df.content_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cosine']  = [phrase_cbow(get_vectors(a), get_vectors(b)) for a,b in zip(df.title_clean, df.content_clean)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['feature'] = [np.hstack((a,b,c)).reshape(1,-1) for a,b,c in df[['title_vec', 'content_vec', 'cosine']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Right on the money</td>\n",
       "      <td>We are using the this book to get 100+ certifi...</td>\n",
       "      <td>[[0.11201899999999998, 0.03991249999999999, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Serves its Purpose!</td>\n",
       "      <td>Couldn't go without it. My 3 1/2 year still we...</td>\n",
       "      <td>[[-0.08785333333333334, -0.022782999999999998,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Trailer Park Bwoys!!!</td>\n",
       "      <td>we get to see it on paramount in ol' LND UK an...</td>\n",
       "      <td>[[0.23044, -0.04867500000000001, 0.431965, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>buyer beware</td>\n",
       "      <td>There are companies selling Bosch knock-offs o...</td>\n",
       "      <td>[[0.10213000000000001, 0.18035, 0.790140000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Great for those cold winters</td>\n",
       "      <td>If you are looking to keep your water liquifie...</td>\n",
       "      <td>[[-0.3699188, 0.3999688, 0.37708379999999997, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                         title  \\\n",
       "0          1            Right on the money   \n",
       "1          1           Serves its Purpose!   \n",
       "2          1         Trailer Park Bwoys!!!   \n",
       "3          0                  buyer beware   \n",
       "4          1  Great for those cold winters   \n",
       "\n",
       "                                             content  \\\n",
       "0  We are using the this book to get 100+ certifi...   \n",
       "1  Couldn't go without it. My 3 1/2 year still we...   \n",
       "2  we get to see it on paramount in ol' LND UK an...   \n",
       "3  There are companies selling Bosch knock-offs o...   \n",
       "4  If you are looking to keep your water liquifie...   \n",
       "\n",
       "                                             feature  \n",
       "0  [[0.11201899999999998, 0.03991249999999999, 0....  \n",
       "1  [[-0.08785333333333334, -0.022782999999999998,...  \n",
       "2  [[0.23044, -0.04867500000000001, 0.431965, -0....  \n",
       "3  [[0.10213000000000001, 0.18035, 0.790140000000...  \n",
       "4  [[-0.3699188, 0.3999688, 0.37708379999999997, ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_set = df.drop(columns = ['title_clean', 'content_clean', 'title_rep', 'content_rep', 'title_vec', 'content_vec', 'cosine'])\n",
    "df_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prince/envs/pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/home/prince/envs/pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "pos_sent = df.loc[df['sentiment'] == 1]\n",
    "neg_sent = df.loc[df['sentiment'] != 1]\n",
    "\n",
    "pos_sent['sentiment'] = [[0, 1] for a in pos_sent['sentiment'].values]\n",
    "neg_sent['sentiment'] = [[1, 0] for a in neg_sent['sentiment'].values]\n",
    "\n",
    "pos_temp = pos_sent.sample(frac = 1)\n",
    "neg_temp = neg_sent.sample(frac = 1)\n",
    "\n",
    "pos_points = len(pos_temp)\n",
    "neg_points = len(neg_temp)\n",
    "\n",
    "train_pos = pos_temp.iloc[:int(pos_points*.7)]\n",
    "train_neg = neg_temp.iloc[:int(neg_points*.7)]\n",
    "train     = pd.concat([train_pos, train_neg])\n",
    "\n",
    "valid_pos = pos_temp.iloc[int(pos_points*.7):int(pos_points*.85)]\n",
    "valid_neg = neg_temp.iloc[int(neg_points*.7):int(neg_points*.85)]\n",
    "valid     = pd.concat([valid_pos, valid_neg])\n",
    "\n",
    "test_pos  = pos_temp.iloc[int(pos_points*.85):]\n",
    "test_neg  = neg_temp.iloc[int(neg_points*.85):]\n",
    "test      = pd.concat([test_pos, test_neg])\n",
    "\n",
    "#pd.concat([a, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.35447   ,  0.07421867, -0.1275    , ...,  0.52613803,\n",
       "         0.27337353,  0.84408413],\n",
       "       [-0.23952   ,  0.31629   ,  0.03467   , ...,  0.43646543,\n",
       "         0.4070336 ,  0.75754095],\n",
       "       [ 0.00687567,  0.29006267,  0.409285  , ...,  0.37515014,\n",
       "         0.19134367,  0.9369619 ],\n",
       "       ...,\n",
       "       [ 0.0384872 ,  0.1234332 ,  0.312714  , ...,  0.44198879,\n",
       "         0.07697718,  0.89275361],\n",
       "       [-0.14392689,  0.53977756,  0.26202944, ...,  0.51986154,\n",
       "         0.18571189,  0.94681355],\n",
       "       [-0.352554  ,  0.092585  ,  0.13877767, ...,  0.47630743,\n",
       "         0.18992584,  0.74379   ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(np.asarray(train.feature.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor = data_utils.TensorDataset(torch.tensor(np.concatenate(np.asarray(train.feature.values), axis = 0)).float(), \n",
    "                                        torch.tensor(np.vstack(train.sentiment.values)))\n",
    "valid_tensor = data_utils.TensorDataset(torch.tensor(np.concatenate(np.asarray(valid.feature.values), axis = 0)).float(), \n",
    "                                        torch.tensor(np.vstack(valid.sentiment.values)))\n",
    "test_tensor  = data_utils.TensorDataset(torch.tensor(np.concatenate(np.asarray(test.feature.values), axis = 0)).float(), \n",
    "                                        torch.tensor(np.vstack(test.sentiment.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = data_utils.DataLoader(train_tensor, batch_size = batch_size, shuffle = True)\n",
    "valid_loader = data_utils.DataLoader(valid_tensor, batch_size = batch_size, shuffle = True)\n",
    "test_loader = data_utils.DataLoader(test_tensor, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "model = nn.Sequential(nn.Linear(201, 100),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(100, 50),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(50, 2),\n",
    "                      nn.Softmax(dim=1))\n",
    "\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1    Training loss: 0.686650\tValidation loss: 0.675602\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 2    Training loss: 0.633938\tValidation loss: 0.568535\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 3    Training loss: 0.515066\tValidation loss: 0.483024\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 4    Training loss: 0.466594\tValidation loss: 0.452953\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 5    Training loss: 0.446026\tValidation loss: 0.437364\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 6    Training loss: 0.432311\tValidation loss: 0.425067\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 7    Training loss: 0.422936\tValidation loss: 0.417928\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 8    Training loss: 0.415083\tValidation loss: 0.411273\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 9    Training loss: 0.408437\tValidation loss: 0.402786\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 10    Training loss: 0.403044\tValidation loss: 0.398436\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 11    Training loss: 0.398386\tValidation loss: 0.393553\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 12    Training loss: 0.394607\tValidation loss: 0.391436\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 13    Training loss: 0.390470\tValidation loss: 0.393573\n",
      "Epoch 14    Training loss: 0.387666\tValidation loss: 0.392158\n",
      "Epoch 15    Training loss: 0.384179\tValidation loss: 0.381248\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 16    Training loss: 0.382174\tValidation loss: 0.381964\n",
      "Epoch 17    Training loss: 0.379478\tValidation loss: 0.375158\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 18    Training loss: 0.376881\tValidation loss: 0.373667\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 19    Training loss: 0.374599\tValidation loss: 0.368626\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 20    Training loss: 0.372258\tValidation loss: 0.394748\n",
      "Epoch 21    Training loss: 0.369967\tValidation loss: 0.374869\n",
      "Epoch 22    Training loss: 0.368107\tValidation loss: 0.369394\n",
      "Epoch 23    Training loss: 0.365628\tValidation loss: 0.365579\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 24    Training loss: 0.364113\tValidation loss: 0.359743\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 25    Training loss: 0.361921\tValidation loss: 0.368965\n",
      "Epoch 26    Training loss: 0.359901\tValidation loss: 0.355236\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 27    Training loss: 0.357730\tValidation loss: 0.354479\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 28    Training loss: 0.355987\tValidation loss: 0.349703\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 29    Training loss: 0.353940\tValidation loss: 0.353602\n",
      "Epoch 30    Training loss: 0.352097\tValidation loss: 0.345735\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 31    Training loss: 0.349845\tValidation loss: 0.346272\n",
      "Epoch 32    Training loss: 0.348268\tValidation loss: 0.359259\n",
      "Epoch 33    Training loss: 0.346471\tValidation loss: 0.344536\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 34    Training loss: 0.344814\tValidation loss: 0.338894\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 35    Training loss: 0.343438\tValidation loss: 0.369020\n",
      "Epoch 36    Training loss: 0.341999\tValidation loss: 0.343879\n",
      "Epoch 37    Training loss: 0.340481\tValidation loss: 0.337874\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 38    Training loss: 0.338931\tValidation loss: 0.335561\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 39    Training loss: 0.338059\tValidation loss: 0.330647\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 40    Training loss: 0.336425\tValidation loss: 0.344937\n",
      "Epoch 41    Training loss: 0.334992\tValidation loss: 0.335865\n",
      "Epoch 42    Training loss: 0.334171\tValidation loss: 0.340295\n",
      "Epoch 43    Training loss: 0.332229\tValidation loss: 0.325069\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 44    Training loss: 0.331499\tValidation loss: 0.328728\n",
      "Epoch 45    Training loss: 0.330411\tValidation loss: 0.328439\n",
      "Epoch 46    Training loss: 0.328829\tValidation loss: 0.335802\n",
      "Epoch 47    Training loss: 0.327575\tValidation loss: 0.357492\n",
      "Epoch 48    Training loss: 0.327335\tValidation loss: 0.318903\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 49    Training loss: 0.326289\tValidation loss: 0.317814\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 50    Training loss: 0.324892\tValidation loss: 0.321917\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "valid_min = np.Inf\n",
    "for e in range(epochs):\n",
    "    running_loss_train = 0\n",
    "    model.train\n",
    "    for features, labels in train_loader:\n",
    "        if train_on_gpu:\n",
    "            features, labels = features.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(features)\n",
    "        loss = criterion(output, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss_train += loss.item()\n",
    "\n",
    "        \n",
    "    running_loss_valid = 0\n",
    "    model.eval\n",
    "    for features, labels in train_loader:\n",
    "        if train_on_gpu:\n",
    "            features, labels = features.cuda(), labels.cuda()\n",
    "        output = model(features)\n",
    "        loss = criterion(output, labels.float())\n",
    "        \n",
    "        running_loss_valid += loss.item()\n",
    "\n",
    "        \n",
    "    print(f\"Epoch {e+1}    Training loss: {running_loss_train/len(train_loader):.6f}\\tValidation loss: {running_loss_valid/len(train_loader):.6f}\")\n",
    "    \n",
    "    if running_loss_valid < valid_min:\n",
    "        print(\"Validation loss decreased. Saving model...\")\n",
    "        torch.save(model.state_dict(), 'deepcbow.pt')\n",
    "        valid_min = running_loss_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model test loss is 0.342851.\n",
      "Model got 12772 out of 15000 correct.\n",
      "Model test accuracy: 85.15%\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0.0\n",
    "model.eval\n",
    "\n",
    "model.load_state_dict(torch.load('deepcbow.pt'))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "for features, labels in test_loader:\n",
    "    if train_on_gpu:\n",
    "        features, labels = features.cuda(), labels.cuda()\n",
    "    output = model(features)\n",
    "    _, pred = torch.max(output, 1)\n",
    "    _, true = torch.max(labels, 1)\n",
    "    loss = criterion(output, labels.float())\n",
    "    test_loss += loss.item()\n",
    "    for a, b in zip(pred, true):\n",
    "        if a.item() == b.item():\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "print(f\"Model test loss is {test_loss/len(test_loader):.6f}.\")\n",
    "print(f\"Model got {correct} out of {total} correct.\")\n",
    "print(f\"Model test accuracy: {correct*100/total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
