{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCL Machine Reading\n",
    "\n",
    "This is an reproduction of the paper https://arxiv.org/pdf/1707.03264.pdf but applied to sentiment analysis of Amazon Reviews.\n",
    "<img src=\"src/uclmr.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Right on the money</td>\n",
       "      <td>We are using the this book to get 100+ certifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Serves its Purpose!</td>\n",
       "      <td>Couldn't go without it. My 3 1/2 year still we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Trailer Park Bwoys!!!</td>\n",
       "      <td>we get to see it on paramount in ol' LND UK an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>buyer beware</td>\n",
       "      <td>There are companies selling Bosch knock-offs o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Great for those cold winters</td>\n",
       "      <td>If you are looking to keep your water liquifie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                         Title  \\\n",
       "0          2            Right on the money   \n",
       "1          2           Serves its Purpose!   \n",
       "2          2         Trailer Park Bwoys!!!   \n",
       "3          1                  buyer beware   \n",
       "4          2  Great for those cold winters   \n",
       "\n",
       "                                             Content  \n",
       "0  We are using the this book to get 100+ certifi...  \n",
       "1  Couldn't go without it. My 3 1/2 year still we...  \n",
       "2  we get to see it on paramount in ol' LND UK an...  \n",
       "3  There are companies selling Bosch knock-offs o...  \n",
       "4  If you are looking to keep your water liquifie...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data\n",
    "\n",
    "df = pd.read_csv(\"data/amazon_reviews_small.csv\", names = ['Sentiment', 'Title', 'Content'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 3 columns):\n",
      "Sentiment    100000 non-null int64\n",
      "Title        99997 non-null object\n",
      "Content      100000 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Title.fillna(\" \", inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the input features for the model, we need to train a count vectorizer model for the content and the title separately to get the term frequency of each feature. We also need to train a TFIDF model on the combination of both.\n",
    "\n",
    "The paper originally mentions getting 5,000 features from the title and the content columns but it would run into a memory error. We instead will depracate to only 3,000 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train count vectorizer for content and title with only the top 5,000 features\n",
    "%timeit\n",
    "count_title   = CountVectorizer(stop_words = 'english',\n",
    "                                max_features = 3000)\n",
    "X_title       = count_title.fit_transform(df.Title.values)\n",
    "\n",
    "count_content = CountVectorizer(stop_words = 'english',\n",
    "                                max_features = 3000)\n",
    "X_content     = count_content.fit_transform(df.Content.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train tfidf values for cosine similarity between title and content\n",
    "%timeit\n",
    "corpus        = list(np.hstack((df.Title.values, df.Content.values)).astype('str'))\n",
    "tfidf         = TfidfVectorizer(stop_words = 'english',\n",
    "                                analyzer   = 'word',\n",
    "                                max_features = 3000).fit_transform(corpus)\n",
    "tfidf         = tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616bba32e9304170af06179260e48863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=100000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Develop array of TFIDF cosine similarity between title and content\n",
    "X_cosine      = []\n",
    "for i in tqdm_notebook(range(len(df))):\n",
    "    X_cosine.extend(cosine_similarity(tfidf[i].reshape(1,-1), \n",
    "                                      tfidf[i+len(df)].reshape(1,-1)))\n",
    "X_cosine      = np.asarray(X_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_title:\t(100000, 3000)\n",
      "X_content:\t(100000, 3000)\n",
      "X_cosine:\t(100000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_title:\\t{X_title.shape}\")\n",
    "print(f\"X_content:\\t{X_content.shape}\")\n",
    "print(f\"X_cosine:\\t{X_cosine.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = hstack((X_title, X_cosine, X_content.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets   = [(a-1) for a in df.Sentiment.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_title, X_content, X_cosine, tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_samp, y_train, y_samp = train_test_split(features, \n",
    "                                                    targets,\n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state = 42)\n",
    "\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_samp,\n",
    "                                                    y_samp,\n",
    "                                                    test_size = 0.5,\n",
    "                                                    random_state =42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (70000, 6001)\tTrain target: 70000\n",
      "Valid shape: (15000, 6001)\tValid target: 15000\n",
      "Test shape:  (15000, 6001) \tTest target:  15000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train shape: {X_train.shape}\\tTrain target: {len(y_train)}\")\n",
    "print(f\"Valid shape: {X_valid.shape}\\tValid target: {len(y_valid)}\")\n",
    "print(f\"Test shape:  {X_test.shape} \\tTest target:  {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor = data_utils.TensorDataset(torch.tensor(X_train.toarray()).float(), \n",
    "                                        torch.tensor(y_train))\n",
    "valid_tensor = data_utils.TensorDataset(torch.tensor(X_valid.toarray()).float(), \n",
    "                                        torch.tensor(y_valid))\n",
    "test_tensor  = data_utils.TensorDataset(torch.tensor(X_test.toarray()).float(), \n",
    "                                        torch.tensor(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = data_utils.DataLoader(train_tensor, \n",
    "                                     batch_size = batch_size, \n",
    "                                     shuffle = True)\n",
    "valid_loader = data_utils.DataLoader(valid_tensor, \n",
    "                                     batch_size = batch_size, \n",
    "                                     shuffle = True)\n",
    "test_loader = data_utils.DataLoader(test_tensor, \n",
    "                                    batch_size = batch_size, \n",
    "                                    shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "model = nn.Sequential(nn.Linear(6001, 100),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(100, 1),\n",
    "                      nn.Sigmoid())\n",
    "\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prince/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/loss.py:512: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "/home/prince/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/loss.py:512: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0    Training loss: 0.333167\tValidation loss: 0.227961\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 1    Training loss: 0.245501\tValidation loss: 0.166292\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 2    Training loss: 0.167280\tValidation loss: 0.082548\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 3    Training loss: 0.079599\tValidation loss: 0.031272\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 4    Training loss: 0.029187\tValidation loss: 0.010400\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 5    Training loss: 0.010047\tValidation loss: 0.004052\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 6    Training loss: 0.004793\tValidation loss: 0.002668\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 7    Training loss: 0.004030\tValidation loss: 0.002378\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 8    Training loss: 0.003702\tValidation loss: 0.002020\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 9    Training loss: 0.002403\tValidation loss: 0.002797\n",
      "Epoch 10    Training loss: 0.001927\tValidation loss: 0.000981\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 11    Training loss: 0.001928\tValidation loss: 0.000767\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 12    Training loss: 0.000832\tValidation loss: 0.000902\n",
      "Epoch 13    Training loss: 0.000862\tValidation loss: 0.000361\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 14    Training loss: 0.000372\tValidation loss: 0.000329\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 15    Training loss: 0.000398\tValidation loss: 0.000234\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 16    Training loss: 0.000268\tValidation loss: 0.000216\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 17    Training loss: 0.000726\tValidation loss: 0.003607\n",
      "Epoch 18    Training loss: 0.005231\tValidation loss: 0.003145\n",
      "Epoch 19    Training loss: 0.001948\tValidation loss: 0.000259\n",
      "Epoch 20    Training loss: 0.000393\tValidation loss: 0.000341\n",
      "Epoch 21    Training loss: 0.000209\tValidation loss: 0.000140\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 22    Training loss: 0.000155\tValidation loss: 0.000127\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 23    Training loss: 0.000174\tValidation loss: 0.000386\n",
      "Epoch 24    Training loss: 0.001218\tValidation loss: 0.002382\n",
      "Epoch 25    Training loss: 0.004548\tValidation loss: 0.002042\n",
      "Epoch 26    Training loss: 0.001658\tValidation loss: 0.000454\n",
      "Epoch 27    Training loss: 0.000608\tValidation loss: 0.000145\n",
      "Epoch 28    Training loss: 0.000144\tValidation loss: 0.000121\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 29    Training loss: 0.000119\tValidation loss: 0.000113\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 30    Training loss: 0.000117\tValidation loss: 0.000102\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 31    Training loss: 0.000106\tValidation loss: 0.000096\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 32    Training loss: 0.000223\tValidation loss: 0.000093\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 33    Training loss: 0.000096\tValidation loss: 0.000088\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 34    Training loss: 0.000089\tValidation loss: 0.000085\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 35    Training loss: 0.000092\tValidation loss: 0.000079\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 36    Training loss: 0.000080\tValidation loss: 0.000070\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 37    Training loss: 0.000071\tValidation loss: 0.000061\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 38    Training loss: 0.000062\tValidation loss: 0.000056\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 39    Training loss: 0.000054\tValidation loss: 0.000050\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 40    Training loss: 0.000049\tValidation loss: 0.000038\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 41    Training loss: 0.000038\tValidation loss: 0.000037\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 42    Training loss: 0.000037\tValidation loss: 0.000036\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 43    Training loss: 0.000036\tValidation loss: 0.000035\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 44    Training loss: 0.000036\tValidation loss: 0.000034\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 45    Training loss: 0.000035\tValidation loss: 0.000034\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 46    Training loss: 0.000697\tValidation loss: 0.000416\n",
      "Epoch 47    Training loss: 0.000512\tValidation loss: 0.000603\n",
      "Epoch 48    Training loss: 0.001944\tValidation loss: 0.000670\n",
      "Epoch 49    Training loss: 0.000306\tValidation loss: 0.000217\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "valid_min = np.Inf\n",
    "for e in range(epochs):\n",
    "    running_loss_train = 0\n",
    "    model.train\n",
    "    for features, labels in train_loader:\n",
    "        if train_on_gpu:\n",
    "            features, labels = features.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(features)\n",
    "        loss = criterion(output, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss_train += loss.item()\n",
    "\n",
    "        \n",
    "    running_loss_valid = 0\n",
    "    model.eval\n",
    "    for features, labels in train_loader:\n",
    "        if train_on_gpu:\n",
    "            features, labels = features.cuda(), labels.cuda()\n",
    "        output = model(features)\n",
    "        loss = criterion(output, labels.float())\n",
    "        \n",
    "        running_loss_valid += loss.item()\n",
    "\n",
    "        \n",
    "    print(f\"Epoch {e}    Training loss: {running_loss_train/len(train_loader):.6f}\\tValidation loss: {running_loss_valid/len(train_loader):.6f}\")\n",
    "    \n",
    "    if running_loss_valid < valid_min:\n",
    "        print(\"Validation loss decreased. Saving model...\")\n",
    "        torch.save(model.state_dict(), 'uclmr.pt')\n",
    "        valid_min = running_loss_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model test loss is 3.706720.\n",
      "Model got 12987 out of 15000 correct.\n",
      "Model test accuracy: 86.58%\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0.0\n",
    "model.eval\n",
    "\n",
    "model.load_state_dict(torch.load('uclmr.pt'))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.cpu()\n",
    "\n",
    "for features, labels in test_loader:\n",
    "    output = model(features)\n",
    "    output = torch.Tensor([1 if a > 0.5 else 0 for a in output])\n",
    "    loss = criterion(output, labels.float())\n",
    "    test_loss += loss.item()\n",
    "    for a, b in zip(output, labels):\n",
    "        if a.item() == b.item():\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "print(f\"Model test loss is {test_loss/len(test_loader):.6f}.\")\n",
    "print(f\"Model got {correct} out of {total} correct.\")\n",
    "print(f\"Model test accuracy: {correct*100/total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
