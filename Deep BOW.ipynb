{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Continuous Bag of Words\n",
    "\n",
    "![alt text](src/arch.jpg)\n",
    "\n",
    "source: https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    }
   ],
   "source": [
    "# Create word2vec dictionary\n",
    "\n",
    "def load_word_vectors(filename):\n",
    "    word_to_index = {}\n",
    "    word_vectors  = []\n",
    "    \n",
    "    with open(filename, encoding=\"utf8\") as fp:\n",
    "        for line in tqdm(fp.readlines(), leave = False):\n",
    "            line = line.split(\" \")\n",
    "            \n",
    "            word = line[0]\n",
    "            word_to_index[word] = len(word_to_index)\n",
    "            \n",
    "            vec = np.array([float(x) for x in line[1:]])\n",
    "            word_vectors.append(vec)\n",
    "            \n",
    "    return word_to_index, word_vectors\n",
    "\n",
    "word_to_index, word_vectors = load_word_vectors(\"model/glove.6B.100d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Right on the money</td>\n",
       "      <td>We are using the this book to get 100+ certifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Serves its Purpose!</td>\n",
       "      <td>Couldn't go without it. My 3 1/2 year still we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Trailer Park Bwoys!!!</td>\n",
       "      <td>we get to see it on paramount in ol' LND UK an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>buyer beware</td>\n",
       "      <td>There are companies selling Bosch knock-offs o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Great for those cold winters</td>\n",
       "      <td>If you are looking to keep your water liquifie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                         title  \\\n",
       "0          2            Right on the money   \n",
       "1          2           Serves its Purpose!   \n",
       "2          2         Trailer Park Bwoys!!!   \n",
       "3          1                  buyer beware   \n",
       "4          2  Great for those cold winters   \n",
       "\n",
       "                                             content  \n",
       "0  We are using the this book to get 100+ certifi...  \n",
       "1  Couldn't go without it. My 3 1/2 year still we...  \n",
       "2  we get to see it on paramount in ol' LND UK an...  \n",
       "3  There are companies selling Bosch knock-offs o...  \n",
       "4  If you are looking to keep your water liquifie...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import csv file of amazon reviews\n",
    "\n",
    "df = pd.read_csv(\"data/amazon_reviews_small.csv\", names = ['sentiment', 'title', 'content'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(phrase):\n",
    "    phrase = phrase.split(' ')\n",
    "    vector = np.zeros((1,100))\n",
    "    word_count = 0\n",
    "    for word in phrase:\n",
    "        try:\n",
    "            vector += np.asarray(word_vectors[word_to_index[word]])\n",
    "            word_count += 1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if word_count > 0:\n",
    "        ans = vector/word_count\n",
    "    else:\n",
    "        ans = np.zeros((1,100))\n",
    "    \n",
    "    return ans\n",
    "    \n",
    "def clean_phrase(phrase):\n",
    "    phrase = phrase.lower()\n",
    "    phrase = phrase.replace(\"'\", \"\")\n",
    "    phrase = phrase.replace(\".\", \"\")\n",
    "    phrase = re.sub(r'([^\\s\\w]|_)+', '', phrase)\n",
    "    return phrase\n",
    "\n",
    "def generate_phrases(dataset):\n",
    "    phrases = dataset[['title', 'content']].dropna().values.flatten().tolist()\n",
    "    return phrases\n",
    "    \n",
    "def generate_tfidf(phrases):\n",
    "    tfidf = TfidfVectorizer()\n",
    "    tfidf.fit(phrases)\n",
    "    return tfidf\n",
    "\n",
    "def phrase_cbow(a, b):\n",
    "    aa = a.reshape(1,-1)\n",
    "    ba = b.reshape(1,-1)\n",
    "    \n",
    "    cos_lib = cosine_similarity(aa, ba)\n",
    "\n",
    "    return cos_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment']     = [(a-1) for a in df.sentiment.values]\n",
    "df['title_clean']   = [clean_phrase(str(a)) for a in df.title]\n",
    "df['content_clean'] = [clean_phrase(str(a)) for a in df.content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = generate_tfidf(generate_phrases(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating clean title\n",
      "Generating clean content\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating clean title\")\n",
    "df['title_rep']     = [tfidf.transform([a]) for a in df.title_clean]\n",
    "print(\"Generating clean content\")\n",
    "df['content_rep']   = [tfidf.transform([a]) for a in df.content_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating clean title vector\n",
      "Generating clean content vector\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating clean title vector\")\n",
    "df['title_vec']     = [get_vectors(a) for a in df.title_clean]\n",
    "print(\"Generating clean content vector\")\n",
    "df['content_vec']   = [get_vectors(a) for a in df.content_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cosine']  = [phrase_cbow(get_vectors(a), get_vectors(b)) for a,b in zip(df.title_clean, df.content_clean)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['feature'] = [np.hstack((a,b,c)).reshape(1,-1) for a,b,c in df[['title_vec', 'content_vec', 'cosine']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Right on the money</td>\n",
       "      <td>We are using the this book to get 100+ certifi...</td>\n",
       "      <td>[[0.11201899999999998, 0.03991249999999999, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Serves its Purpose!</td>\n",
       "      <td>Couldn't go without it. My 3 1/2 year still we...</td>\n",
       "      <td>[[-0.08785333333333334, -0.022782999999999998,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Trailer Park Bwoys!!!</td>\n",
       "      <td>we get to see it on paramount in ol' LND UK an...</td>\n",
       "      <td>[[0.23044, -0.04867500000000001, 0.431965, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>buyer beware</td>\n",
       "      <td>There are companies selling Bosch knock-offs o...</td>\n",
       "      <td>[[0.10213000000000001, 0.18035, 0.790140000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Great for those cold winters</td>\n",
       "      <td>If you are looking to keep your water liquifie...</td>\n",
       "      <td>[[-0.3699188, 0.3999688, 0.37708379999999997, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                         title  \\\n",
       "0          1            Right on the money   \n",
       "1          1           Serves its Purpose!   \n",
       "2          1         Trailer Park Bwoys!!!   \n",
       "3          0                  buyer beware   \n",
       "4          1  Great for those cold winters   \n",
       "\n",
       "                                             content  \\\n",
       "0  We are using the this book to get 100+ certifi...   \n",
       "1  Couldn't go without it. My 3 1/2 year still we...   \n",
       "2  we get to see it on paramount in ol' LND UK an...   \n",
       "3  There are companies selling Bosch knock-offs o...   \n",
       "4  If you are looking to keep your water liquifie...   \n",
       "\n",
       "                                             feature  \n",
       "0  [[0.11201899999999998, 0.03991249999999999, 0....  \n",
       "1  [[-0.08785333333333334, -0.022782999999999998,...  \n",
       "2  [[0.23044, -0.04867500000000001, 0.431965, -0....  \n",
       "3  [[0.10213000000000001, 0.18035, 0.790140000000...  \n",
       "4  [[-0.3699188, 0.3999688, 0.37708379999999997, ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_set = df.drop(columns = ['title_clean', 'content_clean', 'title_rep', 'content_rep', 'title_vec', 'content_vec', 'cosine'])\n",
    "df_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Prince\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "D:\\Users\\Prince\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "pos_sent = df.loc[df['sentiment'] == 1]\n",
    "neg_sent = df.loc[df['sentiment'] != 1]\n",
    "\n",
    "pos_sent['sentiment'] = [[0, 1] for a in pos_sent['sentiment'].values]\n",
    "neg_sent['sentiment'] = [[1, 0] for a in neg_sent['sentiment'].values]\n",
    "\n",
    "pos_temp = pos_sent.sample(frac = 1)\n",
    "neg_temp = neg_sent.sample(frac = 1)\n",
    "\n",
    "pos_points = len(pos_temp)\n",
    "neg_points = len(neg_temp)\n",
    "\n",
    "train_pos = pos_temp.iloc[:int(pos_points*.7)]\n",
    "train_neg = neg_temp.iloc[:int(neg_points*.7)]\n",
    "train     = pd.concat([train_pos, train_neg])\n",
    "\n",
    "valid_pos = pos_temp.iloc[int(pos_points*.7):int(pos_points*.85)]\n",
    "valid_neg = neg_temp.iloc[int(neg_points*.7):int(neg_points*.85)]\n",
    "valid     = pd.concat([valid_pos, valid_neg])\n",
    "\n",
    "test_pos  = pos_temp.iloc[int(pos_points*.85):]\n",
    "test_neg  = neg_temp.iloc[int(neg_points*.85):]\n",
    "test      = pd.concat([test_pos, test_neg])\n",
    "\n",
    "#pd.concat([a, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1086565 , -0.2171745 ,  0.4474775 , ...,  0.55243221,\n",
       "         0.3964709 ,  0.60546891],\n",
       "       [-0.0880905 ,  0.3306007 ,  0.5111311 , ...,  0.50290921,\n",
       "         0.38442631,  0.95579906],\n",
       "       [-0.046564  ,  0.635425  , -0.0936    , ...,  0.53612823,\n",
       "         0.21545359,  0.75013627],\n",
       "       ...,\n",
       "       [-0.218092  ,  0.1300008 , -0.002624  , ...,  0.48524207,\n",
       "         0.23693102,  0.818071  ],\n",
       "       [-0.11760667,  0.41518   ,  0.69020667, ...,  0.45763913,\n",
       "         0.17676815,  0.73917921],\n",
       "       [ 0.01119   ,  0.02407917,  0.28859146, ...,  0.44743207,\n",
       "         0.2434048 ,  0.86634793]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(np.asarray(train.feature.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor = data_utils.TensorDataset(torch.tensor(np.concatenate(np.asarray(train.feature.values), axis = 0)).float(), \n",
    "                                        torch.tensor(np.vstack(train.sentiment.values)))\n",
    "valid_tensor = data_utils.TensorDataset(torch.tensor(np.concatenate(np.asarray(valid.feature.values), axis = 0)).float(), \n",
    "                                        torch.tensor(np.vstack(valid.sentiment.values)))\n",
    "test_tensor  = data_utils.TensorDataset(torch.tensor(np.concatenate(np.asarray(test.feature.values), axis = 0)).float(), \n",
    "                                        torch.tensor(np.vstack(test.sentiment.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = data_utils.DataLoader(train_tensor, batch_size = batch_size, shuffle = True)\n",
    "valid_loader = data_utils.DataLoader(valid_tensor, batch_size = batch_size, shuffle = True)\n",
    "test_loader = data_utils.DataLoader(test_tensor, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "model = nn.Sequential(nn.Linear(201, 100),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(100, 50),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(50, 2),\n",
    "                      nn.Softmax(dim=1))\n",
    "\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0    Training loss: 0.497394\tValidation loss: 0.415503\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 1    Training loss: 0.421199\tValidation loss: 0.436148\n",
      "Epoch 2    Training loss: 0.402765\tValidation loss: 0.420230\n",
      "Epoch 3    Training loss: 0.391127\tValidation loss: 0.380647\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 4    Training loss: 0.379337\tValidation loss: 0.366653\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 5    Training loss: 0.372418\tValidation loss: 0.356158\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 6    Training loss: 0.365166\tValidation loss: 0.356578\n",
      "Epoch 7    Training loss: 0.359796\tValidation loss: 0.346152\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 8    Training loss: 0.355292\tValidation loss: 0.344206\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch 9    Training loss: 0.350383\tValidation loss: 0.352823\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "valid_min = np.Inf\n",
    "for e in range(epochs):\n",
    "    running_loss_train = 0\n",
    "    model.train\n",
    "    for features, labels in train_loader:\n",
    "        if train_on_gpu:\n",
    "            features, labels = features.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(features)\n",
    "        loss = criterion(output, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss_train += loss.item()\n",
    "\n",
    "        \n",
    "    running_loss_valid = 0\n",
    "    model.eval\n",
    "    for features, labels in train_loader:\n",
    "        if train_on_gpu:\n",
    "            features, labels = features.cuda(), labels.cuda()\n",
    "        output = model(features)\n",
    "        loss = criterion(output, labels.float())\n",
    "        \n",
    "        running_loss_valid += loss.item()\n",
    "\n",
    "        \n",
    "    print(f\"Epoch {e}    Training loss: {running_loss_train/len(train_loader):.6f}\\tValidation loss: {running_loss_valid/len(train_loader):.6f}\")\n",
    "    \n",
    "    if running_loss_valid < valid_min:\n",
    "        print(\"Validation loss decreased. Saving model...\")\n",
    "        torch.save(model.state_dict(), 'deepcbow.pt')\n",
    "        valid_min = running_loss_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model test loss is 0.359139.\n",
      "Model got 12621 out of 15000 correct.\n",
      "Model test accuracy: 84.14%\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0.0\n",
    "model.eval\n",
    "\n",
    "model.load_state_dict(torch.load('deepcbow.pt'))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "for features, labels in test_loader:\n",
    "    if train_on_gpu:\n",
    "        features, labels = features.cuda(), labels.cuda()\n",
    "    output = model(features)\n",
    "    _, pred = torch.max(output, 1)\n",
    "    _, true = torch.max(labels, 1)\n",
    "    loss = criterion(output, labels.float())\n",
    "    test_loss += loss.item()\n",
    "    for a, b in zip(pred, true):\n",
    "        if a.item() == b.item():\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "print(f\"Model test loss is {test_loss/len(test_loader):.6f}.\")\n",
    "print(f\"Model got {correct} out of {total} correct.\")\n",
    "print(f\"Model test accuracy: {correct*100/total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
